\chapter{Big Data Technologies}
\label{chapter:environment}

%A problem instance is rarely totally independent of its environment.
%Most often you need to describe the environment you work in, what
%limits there are and so on. This is a good place to do that. Sometimes
%the environment is described together with your own implemantation, in
%the same chapter. Here, we first tell you about the LaTeX working
%environments and then we have an example from an thesis written some years
%ago.

In this chapter, we will examine more carefully the technologies that can be used to implement a stock data pipelines in the big data context and try to answer the second research question.
We have gathered here technologies that we saw used in existing big data stock pipelines in the previous chapter.
We also added couple of promising frameworks that could be used in the pipelines but there is no public information about companies using them yet.
To keep this section compact and more useful for majority of the developers that do not possibly have access to company level resources, we have included only the open-source solutions here leaving outside the services that cloud providers offer such as Amazon S3 for storage or Google Dataflow for ingestion.

\section{Data ingestion}

Here we will be using the same definition for ingestion as before; data fetching and preprocessing which includes data validation and feature extraction.
This is one of the crucial parts of the pipeline as it is responsible of turning arbitrary data into facts that the rest of the pipeline can use and rely on.
This also makes it the hardest part to develop as the developer must understand the data well enough that these parts can work efficiently and prevent erroneous states in the later stages of the pipeline.

We have gathered here three main technologies that are usually mentioned when academics are talking about data ingestion, but in reality all of these frameworks have a bit different tasks they try to fulfill.
This makes comparing these technologies harder and it usually just means that the better technology here is usually the one that is better suited for the current problem, which does not make the other ones any worse than the selected one.
On top of Apache Flume and Apache Kafka which were already used in existing pipelines, we have also included here Apache NiFi which promising features that could also be used for this task in this kind of system.

\subsection{Apache Flume}

Apache Flume is one of the Apache Software Foundation (ASF) projects that we will be seeing a lot especially in the context of ingestion.
From the projects official website we have definition that Flume is "distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data" \cite{flume}.
So the main focus of Flume is to manage log data, but as we have already seen, this is not the only use case for this tool.

Flume was first introduced by Cloudera in 2011.
In same year, it was officially moved under ASF and came out of the incubation phase the following year.
During this incubation, developers had already started to refactor the Flume and the result of this is the 1.X lineage of Flume which is still going to this day. \cite{hoffman}
At the time of writing this the latest production-ready version is 1.9.0.

Flume's high level architecture can be divided into 3 different parts: sources, channels and sinks which all run inside an agent which is an abstraction for one Flume process.
Data is inputted to the system from the sources, then it goes through channels and is finally written into sinks. 
While going through channels the data can be processed using functions that Flume calls Interceptors. \cite{hoffman}

The main unit used of data in Flume is called Event which is structured like most of the other message formats you can find in network protocols.
Event has a header, which is key-value pairs of meta data and a body which usually is the actual data. 
Overall, Flume architecture is message driven which allows it efficiently to multiplex data across multiple computing instances leviating the work load between these machines. \cite{hoffman}

When processing large amounts of data, it is important to avoid bottlenecks that can form in the pipeline and this is why knowing the amount of data flowing through Flume is extremely important.
Bottlenecks that can form in Flume are for example cases where data is coming from sources faster than the Flume is able to write to sinks.
These kind of situations can be avoided with the knowledge of the data domain when configuring Flume instances but also by monitoring Flume processes in order to respond to these kind of situations. \cite{hoffman}

\subsection{Apache Kafka}

Apache Kafka describes itself to be a "distributed publish-subscribe messaging system" and it can be used for three different purposes: as a messaging system, as a storage system and as a stream processor. \cite{kafka}
Because we are examining Kafka in the context of data ingestion, we are mostly interested in its messaging and stream processing capabilities, but it's good to keep in mind that it is possible to store data with Kafka for a longer time if necessary.

Before being a ASF project, it was first developed by LinkedIn to gather user activity data in 2010. \cite{ramalingeswara}
After being released from incubation in 2012 many other big industrial companies such as eBay and Uber \cite{yuan} have taken kafka into use to manage their own enormous data systems.
Today, Kafka is already in its version 2 and can be integrated with almost any modern big data framework. \cite{ramalingeswara}

Kafka operates on publish-subscribe architecture where producers input data into the system by producing data and consumers subscribe to the data which they want to consume/receive.
To make this work with big data, Kafka has a core abstraction called topic which has multiple immutable queues called partitions.
When producers produce new data, this data is appended to a partition queue where where Kafka keeps track which items consumers have already consumed by tracking the offset of a consumer in a queue.
With this kind of architecture Kafka makes promises that the data retains its order throughout the system and does not arrive to consumers out of order.

As for the scalability of this kind of system, a single partition must fit onto a single server, but topic which has multiple partitions does not have this limitation and this allows topics to scale over the Kafka cluster.
Fault-tolerancy can be achieved by just replicating these partitions over the cluster.
Multiple consumers can form consumer groups that consume some topic simultaneously from multiple partitions meaning that in addition of Kafka being itself scalable cluster, it allows its consumers to be also scalable cluster and without any additional complexity. \cite{kafka}

\subsection{Apache NiFi}

Apache NiFi was made to dataflow management and its design is inspired by flow-based programming.
It was originally developed by National Security Agency (NSA) as a system called "Niagara Files" and was moved under ASF in 2014 making it the newest addition under ASF out of these three intoduced ingestion technologies. \cite{bridgwater}

As we have seen already with the last two ingestion frameworks, all these frameworks have their own abstractions for data and the processes that handle this data.
NiFi's core abstraction is FlowFile which represents the data that flows through the system. 
These are processed with FlowFile Processors that are connected to each other by Connections and these can be grouped into Process Groups.
These processors and their connections are then managed by a Flow Controller which acts as the brain of each node in a NiFi cluster. \cite{nifi}

NiFi cluster follows zero-master clustering paradigm meaning that the cluster does not have clear master nodes and vice versa no slave nodes.
Every node in NiFi cluster processes data the same way and the data is divided and distributed to as many nodes as needed.
Apache ZooKeeper is used to handle failure of nodes in the cluster.\cite{nifi}

\section{Data storage}

Data storage forms the center of the pipeline and is one of the major places where bottlenecks can form.
Possible bottlenecks are the query latency and the writing speed which can slow the whole application down if not implemented efficiently enough.
If this non-trivial task was not enough, the data storage must also be able to resist 

\subsection{Hadoop distributed file system}

Hadoop distributed file system (HDFS), which is the core part of Apache Hadoop ecosystem, is one of the current defacto ways to store big data.
It has gained a lot of popularity with the map-reduce programming paradigm.
HDFS build so that it does not have to be run on high quality hardware, normal commodity hardware is more than enough to run the system.\cite{hdfs}

HDFS offers all the same functionalities that a traditional file system would offer.
Clients can create, edit and delete files which can be stored into directories that form hierarchies.
What makes HDFS differ from a normal file system is its ability to handle a lot larger data sets and at the same time have a better fault-tolerance than a traditional file system.\cite{hdfs}

HDFS is based on a master-slave architecture where the master is called NameNode and slaves are called DataNode.
The NameNode stores and manages the state of the whole system and the actual client data never flows through this node.
The data is stored into the DataNodes which manage this data based on the instructions that the NameNode gives them.\cite{hdfs}

For reliability, data is replicated between DataNodes so that the outage of individual DataNodes does not affect the overall perfomance of the system.
In order to identify and react to these kind of failures, each DataNode send heartbeat-like messages to the NameNode, which then conducts needed actions if a heartbeat is missing.
The NameNode itself, on the other hand, is a single point of failure.
It does record the changes and the state of the system into files called EditLog and FsImage which can be used to replay the changes in the system if NameNode goes down temporarily and because of these files are crucial to get the system back up, usually multiple copies of these are stored into disk. \cite{hdfs}

\subsection{Apache HBase}

HDFS by itself is only made to store very large files, so the methods it provides our quite limited.
This is where the Apache HBase comes in which is implemented based on Google's BigTable framework.
Where BigTable uses Google's own file system, GFS, HBase is build on top of HDFS. 
HBase is a NoSQL database altough it does not natively have many features that a normal database would have.
Notably, it does not have its own advanced query language.\cite{george}

HBase's record structure is somewhat similar to its relational counterparts.
Data is stored into rows that consist of columns, that are identified by a unique row key.
Rows form tables and tables can be further grouped into namespaces.
The difference to typical relational data model comes after this.
Columns may have multiple versions and timestamp information about the column and its version are stored into separate entity called cell.
The columns do not form table like structures with rows, and instead act like key-value pairs which can be grouped into column families.
This way values that would be for example 'null' in normal relational database, do not take any room in HBase as such key-value pairs can be omitted. \cite{george}

HBase's core abstraction of scalability is called Region.
Region is a set of continous rows that are split when one Region grows too large.
Each region is served by a single RegionServer and each RegionServer can serve multiple Regions.
So Hbase cluster is scaled by adding these RegionServers which can serve more Regions to clients. \cite{george}

HBase does not instantly store changes into disk. 
Changes are first recorded into a log called write-ahead log (WAL) and after this the change is stored into a memstore which is in memory.
After the changes expire in the memory, the changes in memory are flushed into the disk as they are, into a file called HFile. 
In order to avoid large amounts of small HFiles in the disk, HBase periodically merges these files using a process called compaction.
These are done in file scale (minor compaction), but also in larger scale where all the files inside one region are merged into one and data market for deletion can be cleaned in the process (major compaction). \cite{george}

\subsection{Apache Cassandra}

Apache Cassandra is a NoSQL database that is build on peer to peer architecture.
Cassandra provides its users same tools that a normal relational database would.
Its record structure is the same with rows, columns and tables such as with typical relational database and it provides a SQL-like query language.
What differs Cassandra from a normal relational database is its scalable architecture which we will be looking next. \cite{yarabarla}

Cassandra's main scalable units are called Nodes that are a single instance of Cassandra running in a single machine.
These Nodes are grouped together based on the data they serve and these form Rings.
Data distributed and replicated between the nodes based on the hash of data's partition key.
With consistent hashing algorithm, every Node has the same amount of data. \cite{neeraj}

Unlike HBase, Cassandra does not guarantee the consistency of data due to CAP theorem but instead guarantees the availability at all times.
Cassandra implements "tunably consistent" paradigm where user can define for each write/read to be consistent with the cost of availability.
This high availability, makes Cassandra better choise for example web application where the data must be available at all times, but the data doesn't necessary have to be up to date. \cite{neeraj}


\section{Machine learning frameworks}

When it comes machine learning in big data context, libraries such as Tensorflow and PyTorch are still quite used because of their support of processing data with GPU.
This allows them to perform really well in single machine instances while needing minimal time to develop.
But when these are run in big data context, development becomes a bit more harder as this is not the environment these are designed to run at.

As we saw in the previous chapter, the Apache Spark framework with its Spark ML library is currently the most used big data machine learning platform.
Spark, however, natively supports only classical machine learning models and currently has no ready-made deep learning solutions, which are the state of art methods we are interested in.
We want to still be able to do the deep learning training on Spark environment because of its good integrations with the frameworks we already introduced and its mature ecosystem.
That is why we are next going to briefly look at how the Spark ecosystem works and then move on to see tools which we can use to train deep learning models in Spark ecosystem without writing implementations from scratch.


\subsection{Apache Spark}

Apache Spark is a distributed in-memory data processing system that provides tools for scalable data processing.
Spark has a master-slave architecture, where a driver node acts as a master and executes Spark program through Spark Context.
The driver node passes tasks to worker nodes that the Spark Context together with Cluster Manager manages.
The most popular cluster manager currently seems to be YARN but Mesos or Spark's own standalone could also be used for this job. 
Each worker node then has its own executor process which runs the given tasks in multiple threads when necessary.\cite{spark}

Until version 2.0, Spark's main programming interface was a data structure called Resilient Distributed Dataset (RDD).
RDD is a collection of elements that can be partiotioned throughout the cluster allowing them to be processed in parallel accross multiple servers.
RDD's are still in use for backward compatibility reasons, but from version 2.0 onward Spark's main abstraction has become structure called DataSet.
DataSet is similar to RDD in high level, but it provides richer optimizations under the hood and higher level methods to transform the data.
With the DataSets, a new abstraction called DataFrame was introduced which is can be compared to a table in relational database.
DataFrame is a DataSet of Rows that can be manipulated with similar methods that you could do for DataSet.\cite{spark}

Spark has divided its functionalities into sub-modules that are specified into specific tasks such as Spark Streaming for stream processing and SQL for processing data in tabular form.
We are mostly interested here in Spark ML module.
Spark also has a module called Spark MLlib which some of the research still use.
The main difference between Spark ML and MLlib is that Spark ML provides newer DataFrame API whereas MLlib uses older RDD datastructures.\cite{amirghodsi}

% There is some confusion because of this naming, but according to Spark's own documentation neither of these libraries have been deprecated and Spark ML is just DataFrame extension to MLlib library.
% However, Spark ML is said to be the primary API and no further features are going to be developed in the MLlib API.
% Spark ML is also referred to not be the real name of the DataFrame version, but it has stuck with community because of naming of the packages.
% For general convenience reasons, we are going keep referring the DataFrame version of the machine learning library as Spark ML, but reader should be aware of its relationship with the RDD based library.\cite{spark}

Spark ML provides whole set of classical machine learning algorithms such as linear regression, naive bayesian and random forests.
It also has a concept of pipelines which consist of different transformers and estimators, which are made to make model developing easier.
However, the main problem with Spark ML is that it does not have natively tools to implement deep learning models on Spark.

\subsection{Deeplearning4J}

Deeplearning4J (DL4J) is distributed framework of deep learning algorithms that work on top Spark and Hadoop ecosystems.
It provides all the most popular deep learning models such as multilayered networks, convolutional networks, recurrent networks.
DL4J also provides a lot of tools for preprocessing the data before fed for training in the form of sub-projects. \cite{dl4j}

In academia, DL4J is not that used as its main language is Java, but this choice of language makes it really suitable for industrial use.
However, DL4J supports Keras model import which allows user to use other languages than the JVM based alternatives.
This makes python based prototype systems be able to run in industrial Spark cluster. \cite{dl4j}

\subsection{Apache SystemML}

Apache SystemML is a bit different machine learning system when compared to Spark ML and DL4J.
Similarly to DL4J, SystemML also runs top of Spark, but unlike DL4J it is not a straight forward programming library.
SystemML has its own R- and Python-like declarative machine learning languages (DML), which can be used to define machine learning algorithms that run on the Spark. \cite{systemml}

Currently, these languages cover about the same use-cases that Spark ML does.
What makes SystemML differ from Spark ML is that deep learning models developed in Keras or Caffe can be converted into DML.
So theoretically SystemML supports deep learning through these libaries although its core methods do not have these methods.
This allows it to be run on classical command-line interface, but also from jupyter notebooks which are currently vastly in use in academia. \cite{systemml}

\section{Monitoring}

We have already seen quite a bit of framework specific monitoring solutions, which cover monitoring individual components in the pipeline.
Next we will take this monitoring a step further and look at monitoring solutions that work outside of these components and can be used to monitor the global state of the pipeline.
This can make monitoring easier when all the information is available in one place and it also helps to infer cause-effect relations.

We have gathered here two different monitoring solutions for two different needs.
The ELK stack for monitoring the data flow inside the system and individual components and the ModelDB to specifically monitor the Machine learning models that are produced by the pipeline.

\subsection{Log monitoring}

The most common solution for monitoring logs in bigger system is the ELK stack. 
ELK stack which is an acronym for Elasticsearch, Logstash and Kibana stack, is a common stack used to implement collection of logs and their visualization in Big Data environment.
The need for such as elaborate stack for just collecting logs, comes from the fact that when you have a big data system, the logs of such a system form a big data problem of their own, so in order to solve this problem this stack was developed.
It provides scalable data ingestion system, with a distributed storage that can be accessed and visualized in efficient manner. \cite{elastic}

In this stack, Logtash, an open-source data ingestion pipeline tool, is used to ingest the log data.
This ingested data is the stored into Elasticsearch which is a distributed search and analytics engine, which provides REST interface.
Finally, the data stored into Elasticsearch can be visualized and monitored with Kibana, which is a tool developed to do just this. \cite{elastic}

Because of different user needs the stack has evolved into stack that the company behind these technologies calls Elastic Stack.
This stack really only differs from ELK-stack by a component called Beats, which can be used to build more lightweight stack for simpler needs.
However, pure ELK stack is still very popular option to handle log data. \cite{elastic}

\subsection{Machine learning monitoring}

Open-source monitoring tools that are specifically designed for machine learning are not that common, but couple of tools do exist.
These tools do not only help monitor the models perfomance, but also provide tools for deploying and versioning these models just like developer would use git to manage their code base.

ModelDB is a system developed in Massachusetts Institute of Technology (MIT) for ML model management.
It provides tools that can be used to monitor the performance of models and compare these results with each other.
It also allows logical versioning of models and helps to reproduce the models this way.
ModelDB, however, does not support models from many different ML libraries and currently the only supported libraries are Spark ML and scikit-learn.
The library is said to have second version coming up, but at the time of writing this the library has not had major update for an year.
\cite{modeldb}

Another, newer option is an open-source project called MLFlow.
It does all the same things that the ModelDB does, but markets itself as a more end-to-end solution.
On top of tools for the managing explained in ModelDB paragraph, MLFlow puts more emphasis on packaging the models and the deployment of these models into actual use.
\cite{mlflow}

Unlike ModelDB, MLFlow does not care about what is the library that generates models.
It does this by providing CLI and REST API's that can be integrated with the system ignoring the underlying technologies.
For convenience, it also provides APIs for Python, R and Java which can be used for tighter integration.
MLFlow is as project quite young having its first full version released in June 2019, but it has a healthy development community and is actively developed.
\cite{mlflow}