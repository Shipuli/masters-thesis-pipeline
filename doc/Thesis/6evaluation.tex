\chapter{Evaluating the results}
\label{chapter:evaluation}

%You have done your work, but that's\footnote{By the way, do \emph{not} use
%shorthands like this in your text! It is not professional! Always write out all
%the words: ``that is''.} not enough. 

%You also need to evaluate how well your implementation works.  The
%nature of the evaluation depends on your problem, your method, and
%your implementation that are all described in the thesis before this
%chapter.  If you have created a program for exact-text matching, then
%you measure how long it takes for your implementation to search for
%different patterns, and compare it against the implementation that was
%used before.  If you have designed a process for managing software
%projects, you perhaps interview people working with a waterfall-style
%management process, have them adapt your management process, and
%interview them again after they have worked with your process for some
%time. See what's changed.

%The important thing is that you can evaluate your success somehow.
%Remember that you do not have to succeed in making something spectacular; a
%total implementation failure may still give grounds for a very good master's
%thesis---if you can analyze what went wrong and what should have been
%done. 

 %At this point, you will have some insightful thoughts on your
%implementation and you may have ideas on what could be done in the
%future. This chapter may be combined together with the evaluation
%chapter. All the new insights and findings are given here!  This
%chapter is a good place to discuss your thesis as a whole and to show
%your professor that you have really understood some non-trivial
%aspects of the methods you used\ldots

In this final chapter before conclusions, we will be examining the pipeline which we build in the previous chapter.
We start with basic perfomance metrics that were briefly introduced in section 4.4.
Then we move on to evaluate more management side of the pipeline and finally we end this evaluation with general discussion what could have been done better and what choices possibly turned out to be detrimental.
Finally we close this chapter with a section were we briefly list possible improvements that could still be made to the current pipeline, but could not be done with the resource and time restrictions that this project has.


\section{Perfomance metrics}

The following metrics were taken in a machine that has a Windows 10 as parent operating system, Intel Core i5 2500K processor and 16GB of DDR3 memory.
The docker version that the code was run against was 19.03.2 and it had 3 CPU cores and 10GB of memory as its use.
Not all of these resources were needed, but this was done to in order to, for example, remove the factor of running out of memory that caused inexplicable errors.
These were tested only in one machine which would usually no be enough to make any kind of actual inferences but because the code is run in a containerized environment the role of underlying hardware becomes less important.

\begin{figure}[ht!]
    \includegraphics[scale=0.45]{images/memory1} 
    \centering
    \caption{Resource usage in idle state}
\end{figure}

The resource usage in idle state after the initial data has gone through the pipeline and stored into HDFS can be seen in the figure 6.1.
The figure is the output of the "docker stats" -command.
While inspecting the names of each container one can trivially map these into corresponding components in the architechture figure in section 5.2.
Clarifying the most ambigous namings, the "analyzer" container refers to the standalone DL4J application and build\_connect\_1 refers to the Kafka Connect instance.

The total memory usage here was around 3.8GiB of memory.
This seems to match with the fact that most of the components run on top JVM and we did not have the time to optimize each components garbage collection. 
This is partly because components such as the Kafka Connect, which had a notably large memory usage, would silently fail if not enough memory were offered.
Additionally, this would have probably lead to premature optimization.
For future improvements of the accessibility of the pipeline, this would be a great place to start.

Processor usage in idle state was minimal and did not have any notable jumps except a bit higher usage for Kafka.
The other statistics did not have any notable deviations that would affect greatly on the usage of the pipeline. 
Minor thing to note is that the DL4J application instance is not running actively during these results as user usually are not using both it and the zeppelin instance simultaneously.

\begin{figure}[ht!]
    \includegraphics[scale=0.45]{images/memory2} 
    \centering
    \caption{Resource usage in preprocessing state}
\end{figure}

In figure 6.2 we have the same statistics while the zeppelin code is trying to preprocess the data in order to feed it into LSTM network.
The pipeline has at this point run for some time and the overall memory load has increased to 6.1GiB.
This is mainly due to Zeppelin and Spark worker using a lot more memory.
The CPU usage for both has raised, and there can be seen that at the moment of taking these stats the notebook code is actually using more resources than the actual worker.

As there was not enough time to test other technologies in pipeline these results by themselves do not give us much information about the quality of the software.
They do give us some information about the requirement of the machine that is needed to run a pipeline that is this complex locally.
Over 8GiB of memory is almost a must to run it in active use where the data is flowing continously.
The need for memory can be alleviated with proper garbage collection, but peak memory usage can reach to these numbers.

\section{Pipeline management}

The main benefits of this pipeline do not come from its perfomance.
It is highly containerized meaning it can run with little to none re-configuration on any machine.
During the development of this pipeline the pipeline was developed first on Mac OS and then because of memory requirements moved to Windows machine.
The porting of code did not require any extra steps, other than making sure that the formating of line endings stayed the same with script files that are run in containerized linux environment.

The usage of docker containers in this project also allows the development in isolated networked environment that is very similar to one which would be used in production.
This means that most of the problems that could occur when rolling locally developed code into a production environment where services lie on different machine interconnected by a network, are already solved in here before the user has even started development.
Only thing that the user has to do is to change the addresses in this code from docker based dns to the addresses used in their own cloud environment.

As explained earlier, the technologies such as Kafka Connect have been chosen that changing one techonology does not have large impacts on the overall architecture.
The integrations between components have to be done again in these cases but this has been made as easy as possible.
With the help of docker and docker-compose, most parts of setting up the pipeline have also been automated.
Only in the later part of the pipeline with the analysis, users manual input is required.

Monitoring and especially the Machine Learning pipeline monitoring is made easy with MLFlow.
Although logging new experiments needs a bit of manual work for user because of the nature of MLFlow API, but nevertheless it does make managing the machine learning models easier.
The only downside here is that by default MLFlow does not yet accept DL4J models to be saved in its containerized packages which would make model export easier.

For general monitoring the pipeline is still lacking.
Each component has its own monitoring UI but these are scattered making it hard to follow the bigger picture of the pipeline.
The Kafka Connect component does not even has its own UI, but offers REST API to monitor its metrics.

\section{Discussion}

Choosing DL4J as the machine learning library turned out to be a time consuming option.
After the implementation part, it was clear why data scientist usually use python libraries such as Keras and PyTorch, the amount of resources is substantially larger.
Many of the problems faced during development had quite simple solutions to them, but they were really hard to solve without any reference and empty search results on Google.
This combined with out-of-date examples lead to problems seen in the previous chapter.

One can question whetever we should have opted out from DL4J during development and changed it to some other more compatible library.
We did not do this because there was not that many sensible choices and the documentation and resources online of those that seemed promising were even worse than with the DL4J.
But although the original plan to build to multiple pipelines was not possible partly because of this, we think that the information produced in the implementation chapter is quite valuable and can help a lot of developers who have restrictions on what technologies they can use.

Some can argue that if somebody is building a pipeline in this scale, they probably have cloud resources that they can use to test their code and probably do not need another docker locally.
This is also a valid argument, but they can save a lot of time setting it up with the results of this thesis.
Some might not have this much resources which makes it essential to optimize resource usage especially at the start of the project and the results of the implementation part can help tremendously at this.

Currently, the main method of training machine learning models is to use computers graphics card (GPU) which is multiple times faster than using CPUs.
This makes the current pipeline vastly underperforming when it comes to training machine learning models and this is a clear downside for this system.
It needs quite a bit of CPUs and good networking before it even reaches the perfomance of a single high-end GPU.
The DL4J, does however, support GPU training on spark cluster meaning that adding GPU support to this pipeline with small amount of work is possible.
Unfortunately, we did not have time to implement it as it needs some work for docker to access CUDA resources on parent OS.

Of course, the scope of the implementation part could have been narrowed down as it was quite ambigous.
Especially, during development things that seemed simple turned out to be quite more complex than initially thought.
The inexperience about the technologies used did affect the results, but also tells that the components used were not documented that beginner friendly.
Quite a bit of things were assumed in the documentations, which lead to errors that probably could not had happened with right documentation.

With the resources this project had, the pipeline could not be tested with actual data, which leaves means that we have no prove that the pipeline actually can scale with the data or if this scaling is even possible with the pipeline.
We can only speculate this based on the each of technologies abilities but the possibility for horizontal scalability should be there theoretically.

Because unfortunately, we were only able to try the HDFS as the storage method, the storage is currently lacking a proper query properties meaning that the user has to pull the entire timeseries for each stock.
This in big data context is not desirable behaviour, but integrating a technology like HBase should not need anymore much more work but it is already out of the scope of this project.

\section{Potential improvements}

There are quite a bit of improvements already mentioned at the previous sections as these usually are part of the problems, but we have gathered them and a couple others to this section to give a clearer view on what could be improved.
These are easy improvements but implementing them can take some time.

As stated before, the perfomance of the pipeline could be improved.
This could be done by adding support for GPU usage in Spark cluster which would require the configuration of docker to give the process access to these.
Memory usage could be probably optimized by configuring better carbage collection on components that take most of the memory.

To make the pipeline more automated and improve its mobility, urls for servers could be made dynamic with for example environmental variables that could be configured by the user.
Other such configurations could be made in order to make porting the code to a different environment easier.

The final giveaway in this chapter is that there was quite a bit of things that could have been done better. 
This is partly because of the ambigious scope of the project, inexperience of the author and lack of correct information online.
Despite these, we think that the complications during the development and the final product do offer valuable insights on developing a pipeline for timeseries analysis using these technologies and can give somebody a basis to start developing their own pipeline.
Now we move on to the final chapter where we conclude and summarize everything that we have learned during this thesis.