\chapter{Planning}
\label{chapter:methods}

%You have now stated your problem, and you are ready to do something
%about it!  \emph{How} are you going to do that? What methods do you
%use?  You also need to review existing literature to justify your
%choices, meaning that why you have chosen the method to be applied in
%your work.

Next we will look at how do we can use the technologies in the last chapter in order to build a pipeline that can be used to examine stock data.
We will be reviewing, what we will be doing in the implementation part of this thesis and analyze the information that we have seen to this point.
Finally, we will look how we will evaluate the end-product and how is it better or worse than other alternatives.

As stated in the chapter about obstacles, there are multiple points that can hinder the development of big data pipeline for stock data analysis for novice data scientist.
In this chapter we will start to prove that the current default methology is not suitable for novices and try to find a way which would suit novice data scientist better.
We will be building an example pipeline using methods that would suit novices better than the default methods and show how hard this can be.

We will start by providing information about the options that the novice data scientist have for stock data source in 2019 as this is one of the real requirements for getting started as you need to have some data to analyse.
Then we introduce our way of building a big data pipeline for stock analysis using technologies introduced in the previous chapter.


\section{Data source}

\begin{table}[! htbp]\centering 
    \caption{Stock data sources}
    \begin{threeparttable}
        \begin{tabular}{|p{2cm}|p{2.5cm}|l|p{2cm}|p{3cm}|} 
        \hline
        & Type of data & Price & Data Structure & Restrictions \\ \hline
        IEX \cite{iex}& Intraday and Historical (15 years) & 0\$/month & JSON & 500k datapoints/month \\ \hline
        Alpha Vantage \cite{alphavantage}  & Intraday and Historical (20+ years)& 0\$/month & JSON/CSV & 5 requests/min and 500/day\\ \hline
        World Trading Data \cite{worldtradingdata} & Intraday and Historical (from 1980) & 0\$/month & JSON/CSV & 5 symbols/request, 250 request/day and 25 intraday requests/day\\ \hline
        Intrino \cite{intrino} & Intraday & 52\$/month & JSON / CSV / Excel & 120 request/min\\ \hline
        Quandl \cite{quandl} & Historical $EOD^1$ (from 1996) & $15\$/month^2$ & JSON / Excel & none\\ \hline
        Barchart \cite{barchart} & Intraday and Historical (6 months) & 0\$/month & JSON / CSV / XML & 25 symbols /request and 400 requests/day\\
        \hline
        \end{tabular}
        \begin{tablenotes}\footnotesize
            \item[1] EOD = End of Day
            \item[2] For academic use 
        \end{tablenotes}
    \end{threeparttable}        
\end{table}

Most of the data sources concerning stock data are closed and heavily priced.
However, there exists services that offer data for smaller scale development and analysis.
A list of this kind of services is presented in the table 4.1.
In the table the cheapest possible plan for each service is presented to give a better understanding what kind of possibilities there are for acquiring stock data for analysis purposes and what kind of data formats these offer.

From this table we can make couple of notes.
For aquiring free data test data fast, the IEX provides the best option as its limits are not restricted to time intervals, but the 500k datapoint restriction does not provide nearly enough data for any serious application.
For academic use, Quandl provides at the moment of writing this, the most affortable API for historical data analysis.
Historical data is usually cheaper because with historical data alone you usually cannot make money as stock market revolves around the most recent data, but for training models and other data analysis it is ideal.
Other thing to note is that the amount of historical data can vary greatly between services from 6 months (Barchart) to 39 years (World Trading Data).

For this thesis we have chosen to use data from IEX API which was an open API until 30.09.2019 when the company decided to close this in order to capitalize with the closed API which free plan is in the table.
This data is from the 5 year interval between 2014 and 2019 and the relevant values that it contains are opening prices, closing prices, highest prices, lowest prices and volumes.
In order to test freely with this data, we will implement a simple server that serves this data into our pipeline.

\section{Goal of the implementation part}

As we saw in the first chapter there is a lot of higher level information available about stock pipelines and the technologies they use, but as these are only high level information a lot of practicalities needed to reproduce the pipelines are left out.
There is also a lot of good documentation about how to get started with these technologies but these methods usually are not that sustainable.
What we want to do in the following implementation chapter is to bring this gap down by building a novice friendly pipeline that somewhat reflects what are the pipelines used in reality.

Other important thing we want to highlight is the challenges faced when integrating the possible technologies.
We want to show what are the challenges that novices face and provide information from this in order to prevent these adversities on happening to our possible reader.

The pipeline will not be the best one could build for stock analysis, but the idea is more to show a way to build a novice friendly pipeline and highlight challenges that novice can face while building one.
We will use a lot of ready made products listed in the previous chapter as we do not want to invent the same thing over again when somebody else has already done it better.
So when we say we are building a pipeline, it is more that we are building a configuration to integrate all these pieces together.

\section{Building a novice friendly pipeline}

In the implementation chapter, we will build a pipeline that has two major characteristics to make them sustainable and more novice friendly;
The pipeline will be entirely containerized using Docker and the whole pipeline can be operated using Scala.
In this section we will go through the reasoning of these choices and how we think they solve some of the problems presented in the first chapter.

One of the problems with stock data research was that the pipelines presented were quite hard to reproduce as they were presented from such a high level.
This combined with quickstart tutorials which encourage to run the technologies manually from command line can make it really hard for novice users to develop pipelines that are reproducible and efficient to develop onto.
These are usually not reproducible as they might have a lot of third party dependencies such as java which are only installed into the local computer of the user and these can be inefficient to develop onto as these might need a lot manual steps in order to run the whole system.
To solve this we will be using Docker and Docker Compose to build containerized pipeline.

We chose Docker as the container technology as cloud providers such as Google Cloud Platfrom and Amazon Web Services both support Docker images. \cite{awsdocker} \cite{gcpdocker}
With Docker one can run their pipeline in local or distributed environment allowing the developer to develop their system in realistic distributed virtual environment even though they would not themselves have the resources to use actual cloud service.
This of course fits perfectly to the needs of our target group.

There is nothing new when it comes to containerizing a technology as this has been used as form of development for years.
The problems come from the overall system.
Docker images usually work very well by themselves, but when we start to integrate multiple images and technologies together the problems usually arise.
This is due to the huge amount of different combination of technologies and the continous development of each which can break any of the integration with others.
So each pipeline poses different problems and we try here to show what kind of problems there can be and how to solve them.

We also want the pipeline to be developed using as little different programming languages as possible.
This is to take away the burden of a novice data scientist to learn multiple new languages although one can assume that this kind of person has experience from for example Python.
Although Python is very popular amongst data scientists, we chose to use Scala as most of the open-source Big Data products are build on top of JVM (Java Virtual Machine). 
It has also a good interoperability with Apache Spark and its programming interfaces for both object oriented and functional paradigms.
On top this, Scala offers static typing which is extremely helpful to prevent errors that can occur with long computations saving developers time and resources. \cite{scalabook}
So we think that Scala would be the most beneficial language to use in this type of scenario.

\section{Technology selection}

Next we will explain what technologies we are trying to use in each part of the pipeline.
The choices are highly based on the technologies that have been seen already in use, but there are also choices that seem promising but do not have real-life examples yet.
Although the end product itself is not our main objective here as we want to highlight more about the challenges and the way of building this pipeline, we still want to make the system to be as realistic as possible.

We want the system to be able to ingest normal structured stock data, but have the ability to extend to third-party metadata that can have any form and is usually unstructured.
Because stock data by itself can already scale to gigabytes per day we want the ingestion have the ability to scale with input.
So for the ingestion we will try to use one of the technologies that we introduced in the previous chapter: Apache Flume, Apache Kafka and Apache NiFi.
All of these are scalable data ingestion frameworks that have different paradigms of handling data as seen before.
For only local development these products are a bit heavy weighted, but for scalability these are necessary in order to handle massive amounts of ingested data from varying sources.

The storage should have the ability to scale to the possible terabytes of data.
This means that even when amount of data is enormous, the queries should be executed in somewhat manageable time.
The storage should be fault tolerant in a distributed environment in order to ensure that the data waiting to be analyzed retains its quality throughout the wait.
To fulfill these requirements, we will be testing plain HDFS, Apache Cassandra and, if there is time, Apache HBase.
All of these storage formats have been developed to be used in big data environment and from these HDFS and HBase were both used in some existing pipeline.
HDFS does not have as good as query capabilities that could be hoped for but it offers easy integrations to other technologies and a mature development environment which is perfect for novice developers.
As for Cassandra, we are trying to see whetever it is easy to integrate with the other technologies and can it bring anything to the pipeline with its high availability features.

For the analysis part of the pipeline, the pipeline should be able to preprocess the data for training algorithms that uses it and again the amount of data can be from gigabytes to terabytes.
As we saw in the backgrond chapter, the current cutting edge methods for stock data analysis are deep learning methods.
This is why we want the analysis frameworks have the ability to build and train these models without having to implement them from scratch.

For analysis, there does not exist that many options that work well with our requirements.
The main difficulty here is to keep our pipeline using mainly Scala for programming.
There are two machine learning libraries that have Scala support that they promote and can be used in big data context; Apache Spark ML and Deeplearning4j libraries which were introduced in the previous chapters.
Because deep learning, and specifically LSTM networks, is the current trend in stock analysis, the library needs to have support for these.
Unfortunately, Spark ML does not have these natively as the writing of this, so that leaves us with only Deeplearning4j.

Because building and demonstrating data analysis applications can be a time consuming and complex job, data science community has adopted the usage of Jupyter notebooks when implementing data analysis in Python.
For the same reasons, we will be integrating Apache Zeppelin notebooks into this pipeline as these have a support for Scala programming language and allow submitting applications to Spark cluster.
This is to help with reproducibility of models developed by the users.

Finally, we have the monitoring of the pipeline.
We want to allow the novice data scientist to have a simple and intuitive way to manage and monitor the the state of the whole pipeline.
The main goal is to give the data scientist an ability to notice errors in the pipeline as soon as possible this way preventing the errors to possibly escalate to the latter parts of the pipeline.
We would also want to allow good tracking on progress of machine learning model development in the analysis stage to give the analysist tools to track the results of training.

To monitor the machine learning model development we will integrate the MLFlow tracking into the analysis phase.
This is done by implementing a separate tracking server to allow this process scale separately from the actual analysis.
Thus also enabling responsive tracking UI usage.
If there is time, we will also try to develop a global ELK stack which was described in the previous chapter to give global monitoring on the entire pipeline.

\section{Validation of results}

The main objective of our practical research is to show that the problems described in the first chapter do exist and provide valuable information to novice data scientists on how to build reproducible and sustainable pipelines without a lot of resources.
So the question is, how do we validate the results of this empirical part?

We will be using qualitive analysis focusing on reproducibility of the experiments in the pipeline, sustainability of the pipeline and how novice friendly the pipeline is.
For these we will use same definitions that we have used to this point.
These are not mutually exclusive terms as, for example, a pipeline that is easy to reproduce has a lot of same characteristics as novice friendly pipeline.

By reproducibility of the experiment, we refer to the amount of work and information needed to reproduce the pipeline itself and run an experiment on it.
This is related to the one of the problems with many stock analysis papers that report their pipelines but from a very high level making it hard for example novice data scientists to replicate the experiments.
Here, however, we will not be focusing on the amount of information published, but instead we will focus on how much manual work the user must do to run the pipeline.

We will be using the term "sustainability" again to mean the effort needed to develop the pipeline in the future.
Developing here can be divided into multiple parts, but we use it to refer the process of continuously building new features and keeping the components up to date.
This is related to the problem that there is a lot documentation that shows how to get started but these instructions usually do not make the system sustainable to develop onto.
We will be focusing on the aspect of how much work is needed to upgrade components in the system and how does the system support new features.

Finally, with novice friendliness we mean how much does the user has to know about underlying technologies in order to start their own experiments.
We will be also using this term to analyse how much resources such as money is needed to run the experiments as this is something usually novice data scientists do not have much on their disposal.

We will be comparing the pipeline with two different pipelines using qualitive metrics.
Because of the scarcity of possible points of reference the pipelines have a bit different purposes, but we are more interested how do these pipelines compare with our metrics.
We have picked one pipeline from the academia and one pipeline from the industrial side.
The pipelines are depicted in very high level which can make it hard to make any reasonable deductions, but given the scarcity of data available it is the best we can do.
We are more interested in how they are run than the actual components that they are made of but to give a better picture we also introduce the architectures.

% for screenshots
\begin{figure}[ht!]
    \includegraphics[scale=0.50]{images/example1} 
    \centering
    \caption{Simplified architecture of the academic pipeline}
\end{figure}

The pipeline we have picked from academia is the pipeline proposed in \cite{peng}.
The reported architecture in the paper is presented in figure 4.1.
From the paper we know that the system was ran locally using local Cloudera instance.
Python was used with PySpark as the programming language and the pipeline ran normal machine learning models.
Although this information is not much, we can use it to make some observations on our system.

% for screenshots
\begin{figure}[ht!]
    \includegraphics[scale=0.50]{images/example2} 
    \centering
    \caption{Simplified architecture of the industrial pipeline}
\end{figure}

The industrial pipeline that resembles the most of our pipeline here is the pipeline presented in \cite{snively}.
The high level architecture shown is presented in figure 4.2.
What can be seen in the architecture is that we do not know much about the parts of programs that analyse the data.
Amazon EMR can be used to run almost many different big data frameworks but given that they report using it to analyse data it is higly likely that they are running multiple Spark clusters there.
But what we do know for sure is that the system is run on Amazon Web Services in large scale and we can use this to analyse our own system from the perpectives that we listed above.

We will also be quickly reflecting on the reported challenges.
We will question whetever these are significant in the sense that they could happen to any novice data scientist and is it reasonable to think of them as adversities.
We will be also analysing whetever these challenges could have been avoided and what was the root cause of these challenges.

This concludes this chapter were we examined and reasoned the plans we have for the empirical part of this thesis.
In the next chapter we will report the process of the implementation and see what kind of challenges were faced during it.


%After the we have the implemented pipeline, we need some ways to measure whetever our system fulfills the requirements it has been given and how does it compare with other software that might be as good as it.
%In this section we will introduce the measurements that we will be using in the chapter 6 after the implementation.
%Some of these are quantative such as the performance metrics, but these usually give very shallow view on the system.
%Because of this we will be using more qualitive methods in order to grasp the actual benefits of the pipeline.

%Our focus with the qualitive metrics is going to be on how easy the pipeline is to develop onto and how well the user can monitor changes in the pipeline.
%These are usually important factors when continuously developing a system and can affect greatly when new developers are choosing what pipelines to use.
%This also includes how automated the processes in the pipeline are and does developing and deploying the software require excessive amount of manual work.

%Some quantative metrics are taken in order to asses the fitness of the pipeline for any kind of development where it is important that iterations of the software can be swiftly build and that the whole service can run comfortably on the developers machine.
%Namely for these reasons we will examine the memory consumption of these services on a normal developing machine.
%Other quantative statistics that we can examine are processor usage, network usage and disk usage which, although not as restricting as the memory usage, can tell a lot about the performance of the system.
%For these metrics, we will be using Dockers native tools, and especially the command 'docker stats', to measure the containers usage of resources.

%As there will be a bit different pipelines with different technologies, we can get results that can be used to compare components with one another.
%This would give the reader better information and alternatives on what technologies should they could choose for their own project.
%Now we move on to the implementation chapter where we examine more closely what was done during this thesis.
