\chapter{Introduction}
\label{chapter:intro}

The modern economy revolves around stock market.
Stock market is a way for companies to obtain capital which they can invest into their own business.
In exchange, the person who invests into the companies stocks technically owns a piece of the company which can return profit to the investor two different ways.
The stock can grow in value, which allows the investor to sell the stock in higher price or the company itself can pay dividends to investors based on the number of stocks the investor owns from the company.

The price of the stock is simply determined by the law of supply and demand. 
If somebody is willing to pay a higher price for the stock then the price of the stock can grow.
Because of this the stock market is in continuous fluctuation where people are selling and buying the stocks with the price they think the stock is worth using stockbrokers as the middleman. \cite{person}
All of this has lead to the question, how can we invest most optimally into stocks?
This is where the following computational methods come in. 

There are many strategies on how to invest into these stocks which depend on multiple factors such as; how much do you expect to profit with your investment, how much are you willing to take risk, do you want to make money by selling the stocks or by receiving dividends and so on.
The underlying principle with every strategy is to minimize the risk you need to take in order to gain as much as profit as possible.
Some of the strategies are based on subjective evaluation of the companies, but more technical strategies use metrics that are calculated from the financial statistics or the real-time market values.
Strategies using the former data are called fundamental analysis and the strategies using latter data technical analysis.
Neither of these approaches can predict the future of the market, but can statistically decrease the probability of larger losses in the market for the investor altough the probability of large losses is still not zero with these methods. \cite{fox}

Fundamental analysis is based on the idea that each stock has a intrinsic value that can be larger than the actual price of the stock in the market and buying these will eventually lead to profits.\cite{sohnke}
The fundamental analysis focuses on the financial metrics that consist of companys overall statistics.
These are for example how much the company has made profit, how much the company has paid dividends and what is companys cash flow.
These tell a lot about the growth of the company and how the future of the company looks like.
These metrics are usually published quarterly four times a year and present more long-term statistics about the company.
Because of this, the amount of data these values present is quite small in terms of space.

The technical analysis that focuses on the real-time market values, on the other hand, needs new data almost daily.
Stock exchanges are usually open from morning, opening around 8 to 10am, until evening, closing around 5 to 7pm on weekdays.
Before and after this there are more limited pre- and after-hours trading which lasts usually around 1 to 2 hours depending on the exchange in which more limited stock trades can be made.
During these hours multiple values are recorded on the prices of the stock from which the most important ones being: the highest price the stock was sold, the highest price the stock was sold and the number of stocks traded during the time interval.
The technical analysis focuses on finding recognizable patterns through this data. \cite{murphy}
Where the data used by the fundamental analysis was relatively small, these values can generate gigabytes of raw data in a week.

Developing a system that can be used to conduct technical analysis means that the system should be planned to be able to handle large amounts of these data as time progresses.
As such task is not trivial, the goal of this thesis is to provide developers and researchers, who want to analyse this data efficiently, basic knowledge and tools on what are the best current solutions on handling this data.
With this knowledge these data scientists can save considerable amount of time without the need of trial and error when developing this kind of system from the ground up.

% for screenshots
\begin{figure}[ht]
    \includegraphics[scale=0.26]{images/system2} 
    \centering
    \caption{Example of a stock data pipeline}
\end{figure}

\section{Application scenario}
% research and engineering questions (why do you have to do it and why it is new)

To have a better picture what are the challenges of developing a system for technical analysis are, we will next look at a example case.
This is to give the reader more complete view of a system that handles stock market data analysis.
After this, for the rest of the thesis, we are going to focus on a part of this system, but the purpose of this example is to give a better view of the system as a whole in order to understand some of the requirements that these other components give to the focus of this thesis.

Somewhat normal system for analysing both fundamental and technical analysis is presented in figure 1.
In the figure, the components marked with solid lines represent the core system functions, and the dashed lines represent add-on functionalities that should be possible to extend into the system in the future.
The requirements of the system are usually as follows;
The system should produce long ($r_1$) and short ($r_2$) term predictions of stock prices.
The computation of long term predictions can take time because of the nature of these predictions but the short term predictions should be between two minutes to one hour available.
Long term predictions are the result of fundamental analysis ($f_2$) and historical technical analysis ($f_4$) whereas the short term prediction should come mostly from quick technical analysis ($f_3$).
The cost of the system should grow logarithmically/linear with time meaning that the cost of processing and storing data should not exponentially increase over time.
Finally, the core system should be able to fulfill these requirements for at least the +5000 companies in the major U.S stock markets which is the most accessible and individually significant market.

The data to the application can be ingested from two main types of data sources quote and fundamental.
These sources consist of values that were briefly described previously in technical and fundamental analysis respectively.
The quote data is usually updated with minute intervals depending on the provider whereas the fundamental data does not change so often.
Theoretically, the fundamental data can change anytime, because of dividends which can be payed whenever the companies want but this does not happen often and these values are mostly used in the long term fundamental analysis so longer update intervals are acceptable.
In the figure, these are separated into the single market ones ($d_1$ and $d_2$), which represent the minimum of at least including the U.S stock market and the other sources ($d_3$ and $d_4$) that provide the same data on other global markets.

In the figure, the extendable global data sources are grouped into one box but in reality this data would be ingested from numberable different providers as there is no single entity at the time of writing this that provides all of this data.
Theoretically the minimum of a maximum size of this extendable data would be 5Gb per day which is extrapolated in from the U.S market data based on statistics that in January 2019 there were globally 51 599 companies listed in the stock markets \cite{global}.
This amount can and will fluctuate as companies enter and exit the markets but it gives us the scale of data we are working with.

Today, stock market analysis has also a large focus on predicting stock prices using secondary data sources that can have reflect and affect the prices of stocks. 
These secondary data sources can be anything but at the moment one of the most researched sources are traditional media and social media data.
Examples of using this kind of data to predict predict stocks can be found in \cite{kao}, \cite{skuza} and \cite{wai}.
This is why the system should have the ability to extend to ingest data from arbitrary secondary sources ($f_6$ and $d_5$ in the figure) to provide more versatile predictions about the stocks.
The amount of this data can be unlimited but is restricted to relevant sources.

Data is usually ingested from these data sources mainly using HTTP-protocol as this is the main method that these services ($d_1$ - $d_4$) provide.
Other possible methods that are usually available are for example Excel sheets and sometimes websockets, of which the websockets can be actually useful in cloud system, but as the amount of data grows more custom file transfer methods are used.
However, as the HTTP-methods are currently the most publicly available technology, this thesis is going to focus on these.
The main ingestion functions are usually separated into two main types of functions $f_1$ and $f_2$. 
$f_1$ is constantly polling and processing data whereas $f_2$ handles batch processing.
Both of these function usually store their raw output into the storage, but $f_1$ passes this also to the immediate technical analysis.

The system should usually have two types of methods on analysing data.
For methdos that allow streaming updates there is $f_3$, which can for example be cumulative/reinforced ML models and for methods that need historical data in order to calculate the prediction, there is $f_4$.
For fundamental analysis, there is no a specific function as the introduced data sources usually provide these values pre-calculated and these values are usually easy to calculate dynamically with little to none amount of processing.

Finally, at the center of the system, there is the storage which is used to store the calculated predictions as well as the raw data from the data sources for later analysis.
For historical technical analysis, $f_4$, the storage should provide reasonable range query times when quering historical data and for the results the storage should provide efficient point queries for the results ($<$ 1s).

\section{Audience and Research Questions}

We saw the possible complexity of a stock data analysis pipeline in the previous section.
There are a lot of articles about how to build a stock analysis pipeline but these usually are focused on the analysis of small amount of data in order to test some hypothesis.
Building a large-scale stock data analysis pipeline is a vastly complex task and to alleviate this process, this thesis plans to provide tools and information on how to build one.

This thesis is aimed at novice data scientists that work or want to work with stock market data, but do not have any experience in big data technologies.
We define the term "novice" here to mean persons who have knowledge about analysing data using modern data analysis methods and know how to conduct this onto small datasets, but their proficiency lies strictly on the data analysis side.
So the reader can be very good at analysing data, but lacks proficiency at the engineering side of big data technologies.
The reason why the reader would want to read this could be because they have interest in the subject or are, for example, working for a startup which does not have resources to hire multiple persons with high degree on the subject.
This thesis is meant for this kind of people, as the place to find easily knowledge on how to get started.

As the system can be very complex as seen in the previous section, the focus of this thesis is going to be the part of the pipeline that calculates models based on historical data.
This means the ingestion ($f_1$), storage ($f_5$) and analysis ($f_4$) of the historical data in a environment where the time restrictions are not that tight but the amount of data is enormous.
We want the pipeline to be able to have the possibility to ingest meta data from third-party sources as seen on the example case in the previous section.
We have chosen this part of the pipeline as this is the part that has the highest probability to scale to big data first and is the part of the pipeline that is usually referred when talked about data analysis on stock market data.

The main goal of this thesis is going to be to make the data analysis in this big data context as accessible as possible within the time constraints and resources that this thesis has.
What we mean by the word "accessible" here is that we want to minimize the time needed for the novice data scientist to start experimenting with their analysis, while making sure that developing models can continue smoothly in the future.
In this sense, things that make the process of analysing stock data in big data context "inaccesible" are things such as scattered information about options, outdated information online and lack of information on how to integrate different parts together.
This thesis plans to alleviate these aspects by providing aggregated information, testing and software that can be used to navigate through these different problems.

%As the field of possible technologies is large, the main focus of this thesis would be the comparison of current relevant technologies in the context of stock data to help novice data scientists to decide what technologies to possibly use in their own projects.
%The main result of this thesis would be information and possible tools that developers could use to develop this kind of system more efficiently.
%Developers here can be people from companies that either do stock analysis as their main business or just want to analyze stock data efficiently.
%These results could also be used in research to implement analyzing pipelines more efficiently so that the research group can focus on the analyzing of the stock data instead of worrying with getting the data to these parts.

%Analyzing the stock data is also what most of the research today is focused on as this is the part that can actually produce profit.
%This means that most papers ignore the steps of ingesting and storing this data.
%Examples of this kind of papers are \cite{wu}, \cite{aghakhani} and \cite{kao}.
%So one of the goals of this thesis would be to bring more comprehensive picture of stock data pipelines. 

This thesis will approach this subject from three different perpectives that cumulate on one another.
These perspectives are represented by the following three research questions:

What kind of pipelines are currently used in stock market data analysis in big data context?
What are the methods and tools currently used to conduct technical analysis and possibly what kind of data they use.
This thesis plans to aggregate this scattered information into more available format so that the reader can more easily read about the subject.

How do these pipelines compare in the perspective of novice data scientist?
As there are enormous amount of different technological frameworks, this thesis plans to provide the reader information on the most promising ones currently and do a comparison between them so that user can have some point of reference when choosing technologies on their own.

Which one is the best for novice data scientist as a starting point?
Finally the thesis plans to provide a prototype implementations of stock data pipelines using the technologies that seem most prominent.
These are compared with one another using relevant metrics that reflect how manageable and easy it is to develop on the implemented pipeline.
If the prototype system fits readers developed architecture it can be used as a basis for further development by the reader
This is to save time of a person developing these when the trial and error has been done beforehand.

\section{Expected Outcome}

For the first research question "What kind of pipelines are currently used?" this thesis plans to provide a study on currently used stock market analysis methods and used pipelines.
The outcome will be a literature study on pipelines that are used in academia and industry based on public information.

For the second question "How do these pipelines compare in the perspective of novice data scientist" the thesis would perform an analysis on the most promising big data pipeline technologies.
This will be done in a form of comparison of different types of solutions that are either used in practice based on the results in the literature study for the first question or just seem promising for the use case.
The result of this part could be used to decide what seems to be the most suitable technology to use to implement the stock analysis pipeline technically.

For the last question "Which one is the best for novice data scientist as a starting point?" the thesis would implement open-source prototype solutions based on the results of the second research question.
The expected outcome is to find a pipeline that a novice data scientist could develop onto which at the same time fulfills the aforementioned requirements.
These prototypes could be used by anyone (company or individual) as it is or as a base to build a more complex system on top of them or use them as it is to perform data analysis.

\section{Structure of the Thesis}

In the first chapter of the thesis we will be focusing on solving the first research question "What kind of pipelines are currently used?" by offering aggregated background knowledge on the subject.
The thesis will start by going through scientific papers about stock markets and stock analysis.
We will examine trends and state-of-art methods on stock analysis that have been used in recent scientific research.
Then we move on to examine what kind of pipelines exists in real-life.
We will be examining both commercial and research pipelines in order to give a more complete picture on options that exist.
We will also perform a literary research on technologies that are currently used to perform big-data ingestion, storing and analysis, selecting from the list of technologies mostly those that have been seen to be used in practice in this context while introducing couple of promising ones.

In the next chapter we will be focusing on the second research question "How do these pipelines compare in the perspective of novice data scientist?" and perform an comparison of the technologies introduced in the previous chapter.
This will be used also as a preparation for the experimental part of the thesis by choosing the technologies that we are going to try to use in practice.
After we have seen how these technologies compare to one another we start the experimental part by defining what are we going to be building and how we are going to evaluate it.
All this in preparation to answer the final research question "Which one is the best for novice data scientist as a starting point?".

We will then start the actual implementation part.
In the following chapter, we will describe what was done and what challenges we faced while implementing the systems.
The chapters from this on will be focused on the technical and practicalities that appeared during development.

After this chapter we move on to evaluate the results that we got from the implementation chapter. 
We will try our best to evaluate the quality of solutions and bring up fair criticism that the end result could possibly have.
We then finish this chapter with a discussion on what could have been done better and how the systems developed could be improved in the future.
Then finishing the thesis we will have a conclusion chapter that summarizes the results of the thesis.
However, next we start the thesis by examining the field of stock data analysis.
