Flume cassandra no good integrations
=> plugins unmaintained
=> rip

DBs need processing jobs in order to digest from kafka

Docker images are not that used? Newer thing that can help development when this logic is decoupled from the OS

Big data europe has made docker containers for hadoop ecosystem

Uses traefik which is not needed

Kafka does not have easy ways to create topic
=> most production ready way is to use AdminAPI and create a new application

Kafka has no web ui to check what is up
=> https://stackoverflow.com/questions/49276785/monitoring-ui-for-apache-kafka-kafka-manager-vs-kafka-monitor/49292872
=> TL;DR not many "good" non-commercial solutions


Errors while developing
- Cache problems => Errors that made no sense (file corruption)
- Not enough memory => Silently killing processess (scala)
- Kafka connect => Worker is killed but process continues (aka Kafka HDFS integration)
  - Namenode did not have enough time to setup => inf while loop to ping namenode
  - Connection error: Protocol message end-group tag did not match expected tag. => wrong port (right port 9000)
  - Wrong serializer => JSON when actually String needed
  - Cannot read files from HDFS 
  - org.apache.avro.AvroRuntimeException: already open => flush size set to 1 with one partition https://github.com/confluentinc/kafka-connect-hdfs/issues/298
- Spark HDFS integration
  - Avro package in sbt dependencies was not enough => had to use --packages flag
  - https://issues.apache.org/jira/browse/SPARK-27623 => downgrade package
  - https://datameer.zendesk.com/hc/en-us/articles/213151023-java-nio-channels-SocketChannel-connection-pending-remote-IP-port- => add config to spark context
  - java.nio.channels.UnresolvedAddressException (no other information given) => More information with spark Debug flags => HDFS uses docker random generated hashes for hostnames => No solution => stab /etc/hosts
  - Exception in thread "main" org.apache.spark.sql.avro.IncompatibleSchemaException: Found recursive reference in Avro schema, which can not be processed by Spark: => Avro schema is automatically generated by kafka-connect and this does not work with Spark => change save file format to json (side effect: code no longer errors from docker hashes)

Other notes:
- Spark still does not support latest scala version (current: 2.13, supported: 2.12)

TODO problems:
- HDFS permissions (?) do not allow web ui file downloads


Bits of info that probably should be kept safe:
Kafka data isn't indexed by key, it's partitioned by it,